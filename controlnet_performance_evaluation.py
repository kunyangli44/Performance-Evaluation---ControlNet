# -*- coding: utf-8 -*-
"""ControlNet Performance Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17qUktJquodyctTpA-HrRl4wEIcrtVUbP

To process different conditionings depending on the chosen ControlNet, we also need to install some
additional dependencies:
- [OpenCV](https://opencv.org/)
- [controlnet-aux](https://github.com/patrickvonplaten/controlnet_aux#controlnet-auxiliary-models) - a simple collection of pre-processing models for ControlNet

We will use the famous painting ["Girl With A Pearl"](https://en.wikipedia.org/wiki/Girl_with_a_Pearl_Earring) for this example. So, let's download the image and take a look:
"""

from diffusers import StableDiffusionControlNetPipeline
from diffusers.utils import load_image
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

# load three bird images in current directory
link = "./"
images = [
    load_image(link + "bird.jpg"),
    load_image(link + "bird_paint.jpeg"),
    load_image(link + "bird_sketch.jpeg"),
]

def create_resolutions(images):
    """
    Creates different resolutions for a list of images.

    Args:
        images: A list of PIL Images.

    Returns:
        A dictionary where keys are image indices and values are dictionaries
        containing different resolutions of the corresponding image.
    """

    image_resolutions = {}
    for name, image in zip(["Photo", "Painting", "Sketch"], images):
        original_image = image.copy()
        width, height = original_image.size

        # Create resized images
        resized_images = {
            "original": original_image,
            "divided_by_2": original_image.resize(
                (width // 2, height // 2), Image.Resampling.LANCZOS
            ),
            "divided_by_4": original_image.resize(
                (width // 4, height // 4), Image.Resampling.LANCZOS
            ),
            "divided_by_8": original_image.resize(
                (width // 8, height // 8), Image.Resampling.LANCZOS
            ),
        }

        image_resolutions[name] = resized_images

    return image_resolutions

# prompt: # For all image in images, make four types of resolutions: original, divided by 4, divide by 8, divide by 12, stored as dict

# ... (Your existing code)




# Example usage (assuming 'images' is defined as in your provided code):
image_resolutions = create_resolutions(images)

# plot image_resolutions with plt
fig, axes = plt.subplots(3, 4, figsize=(15, 10)) # Changed to 3 rows, 4 columns

for i, (key, value) in enumerate(image_resolutions.items()):
    for j, (subkey, subvalue) in enumerate(value.items()):
        row = i
        col = j
        axes[row, col].imshow(subvalue)
        axes[row, col].axis('off')
        axes[row, col].set_title(f"{subkey} {key}")
plt.tight_layout()
plt.show()

"""Next, we will put the image through the canny pre-processor:"""

import cv2
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

# ... (Your existing code, including the definition of 'images' and 'image_resolutions')

def create_canny_images(image_resolutions):
    """Applies Canny edge detection to images of different resolutions."""
    low_threshold = 100
    high_threshold = 200
    canny_images = {}

    for i, resolutions in image_resolutions.items():
      canny_images[i] = {}
      for resolution_name, image in resolutions.items():
          image_np = np.array(image)
          image_np = cv2.Canny(image_np, low_threshold, high_threshold)
          image_np = image_np[:, :, None]
          image_np = np.concatenate([image_np, image_np, image_np], axis=2)
          canny_image = Image.fromarray(image_np)
          canny_images[i][resolution_name] = canny_image
    return canny_images


canny_images = create_canny_images(image_resolutions)

# Display Canny images in a grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))

for i, (key, value) in enumerate(canny_images.items()):
    for j, (subkey, subvalue) in enumerate(value.items()):
        row = i
        col = j
        axes[row, col].imshow(subvalue)
        axes[row, col].axis('off')
        axes[row, col].set_title(f"Canny - {subkey} {key}")

plt.tight_layout()
plt.show()

from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16
)

from diffusers import UniPCMultistepScheduler

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
pipe.enable_xformers_memory_efficient_attention()

import matplotlib.pyplot as plt

# ... (Your existing code)

# Assuming 'canny_images' is defined as in your previous code

# Display generated images in a 3x4 grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))
image_index = 0

for i, (key, value) in enumerate(canny_images.items()):
    for j, (subkey, canny_image_original) in enumerate(value.items()):
        prompt = ", best quality, extremely detailed"
        prompt_list = [t + prompt for t in ["bird"]]
        generator = [torch.Generator(device="cpu").manual_seed(10) for _ in range(len(prompt_list))]
        output = pipe(
            prompt_list,
            canny_image_original,
            negative_prompt=["monochrome, lowres, bad anatomy, worst quality, low quality"] * len(prompt_list),
            generator=generator,
            num_inference_steps=20,
        )

        row = i
        col = j
        axes[row, col].imshow(output.images[0])
        axes[row, col].axis('off')
        axes[row, col].set_title(f"Generated - Canny - {subkey} {key}")
        image_index += 1

plt.tight_layout()
plt.show()

"""Another exclusive application of ControlNet is that we can take a pose from one image and reuse it to generate a different image with the exact same pose. So in this next example, we are going to teach superheroes how to do yoga using [Open Pose ControlNet](https://huggingface.co/lllyasviel/sd-controlnet-openpose)!

First, we will need to get some images of people doing yoga:
"""

link = './'
imgs = [
    load_image(link + 'man.jpg'),
    load_image(link + 'man_paint.jpeg'),
    load_image(link + 'man_sketch.jpeg'),
]

# do resolution changes to imgs
image_resolutions = create_resolutions(imgs)

# Display Canny images in a grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))

for i, (key, value) in enumerate(image_resolutions.items()):
    for j, (subkey, subvalue) in enumerate(value.items()):
        row = i
        col = j
        axes[row, col].imshow(subvalue)
        axes[row, col].axis('off')
        axes[row, col].set_title(f"Pose - {subkey} {key}")

plt.tight_layout()
plt.show()

"""Now let's extract yoga poses using the OpenPose pre-processors that are handily available via `controlnet_aux`."""

from PIL import Image
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
import torch
from controlnet_aux import OpenposeDetector
from diffusers.utils import load_image

# ... (Your existing code, including the definition of 'images' and 'image_resolutions')

def create_pose_images(image_resolutions):
    """Applies OpenPose detection to images of different resolutions."""
    low_threshold = 100
    high_threshold = 200
    model = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')
    open_poses = {}

    for i, resolutions in image_resolutions.items():
      open_poses[i] = {}
      for resolution_name, image in resolutions.items():
          pose = model(image)
          open_poses[i][resolution_name] = pose

    return open_poses

poses = create_pose_images(image_resolutions)

# Display Canny images in a grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))

for i, (key, value) in enumerate(poses.items()):
    for j, (subkey, subvalue) in enumerate(value.items()):
        row = i
        col = j
        axes[row, col].imshow(subvalue)
        axes[row, col].axis('off')
        axes[row, col].set_title(f"Pose - {subkey} {key}")

plt.tight_layout()
plt.show()

controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-openpose", torch_dtype=torch.float16
)

pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

# Remove if you do not have xformers installed
# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers
# for installation instructions
pipe.enable_xformers_memory_efficient_attention()

pipe.enable_model_cpu_offload()

# Display generated images in a 3x4 grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))
image_index = 0

for i, (key, value) in enumerate(poses.items()):
    for j, (subkey, pose) in enumerate(value.items()):
        prompt = ", best quality, extremely detailed"
        prompt_list = [t + prompt for t in ["An old man sitting"]]
        generator = [torch.Generator(device="cuda").manual_seed(10) for _ in range(len(prompt_list))]
        output = pipe(
            prompt_list,
            pose,
            negative_prompt=["monochrome, lowres, bad anatomy, worst quality, low quality"] * len(prompt_list),
            generator=generator,
            num_inference_steps=20,
        )

        row = i
        col = j
        axes[row, col].imshow(output.images[0])
        axes[row, col].axis('off')
        axes[row, col].set_title(f"Generated - OpenPose - {subkey} {key}")
        image_index += 1

plt.tight_layout()
plt.show()

"""Throughout the examples, we explored multiple facets of the [`StableDiffusionControlNetPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet) to show how easy and intuitive it is play around with ControlNet via Diffusers. However, we didn't cover all types of conditionings supported by ControlNet. To know more about those, we encourage you to check out the respective model documentation pages:

* [lllyasviel/sd-controlnet-depth](https://huggingface.co/lllyasviel/sd-controlnet-depth)
* [lllyasviel/sd-controlnet-hed](https://huggingface.co/lllyasviel/sd-controlnet-hed)
* [lllyasviel/sd-controlnet-normal](https://huggingface.co/lllyasviel/sd-controlnet-normal)
* [lllyasviel/sd-controlnet-scribble](https://huggingface.co/lllyasviel/sd-controlnet-scribble)
* [lllyasviel/sd-controlnet-seg](https://huggingface.co/lllyasviel/sd-controlnet-scribble)
* [lllyasviel/sd-controlnet-openpose](https://huggingface.co/lllyasviel/sd-controlnet-openpose)
* [lllyasviel/sd-controlnet-mlsd](https://huggingface.co/lllyasviel/sd-controlnet-mlsd)
* [lllyasviel/sd-controlnet-mlsd](https://huggingface.co/lllyasviel/sd-controlnet-canny)

We welcome you to combine these different elements and share your results with [@diffuserslib](https://twitter.com/diffuserslib). Be sure to check out [the Colab Notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/controlnet.ipynb) to take some of the above examples for a spin!

We also showed some techniques to make the generation process faster and memory-friendly by using a fast scheduler, smart model offloading and `xformers`. With these techniques combined the generation process should take only ~3 seconds on a V100 GPU and consumes just ~4 GBs of VRAM for a single image ⚡️
"""

link = './'
imgs = [
    load_image(link + 'trailside.png'),
    load_image(link + 'trailside_paint.jpeg'),
    load_image(link + 'trailside_sketch.jpeg'),
]

image_resolutions = create_resolutions(imgs)

fig, axes = plt.subplots(3, 4, figsize=(15, 10))

for i, (key, value) in enumerate(image_resolutions.items()):
    for j, (subkey, subvalue) in enumerate(value.items()):
        row = i
        col = j
        axes[row, col].imshow(subvalue)
        axes[row, col].axis('off')
        axes[row, col].set_title(f"Depth - {subkey} {key}")

plt.tight_layout()
plt.show()

from transformers import pipeline
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from PIL import Image
import numpy as np
import torch
from diffusers.utils import load_image

# ... (Your existing code, including the definition of 'images' and 'image_resolutions')

def create_depth_images(image_resolutions):
    """Applies Depth detection to images of different resolutions."""
    depth_estimator = pipeline('depth-estimation')
    depth_images = {}

    for i, resolutions in image_resolutions.items():
      depth_images[i] = {}
      for resolution_name, image in resolutions.items():
          image = depth_estimator(image)['depth']
          image = np.array(image)
          image = image[:, :, None]
          image = np.concatenate([image, image, image], axis=2)
          image = Image.fromarray(image)
          depth_images[i][resolution_name] = image

    return depth_images

depth_images = create_depth_images(image_resolutions)

# Display Canny images in a grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))

for i, (key, value) in enumerate(depth_images.items()):
    for j, (subkey, subvalue) in enumerate(value.items()):
        row = i
        col = j
        axes[row, col].imshow(subvalue)
        axes[row, col].axis('off')
        axes[row, col].set_title(f"Depth - {subkey} {key}")

plt.tight_layout()
plt.show()

controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-depth", torch_dtype=torch.float16
)

pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

# Remove if you do not have xformers installed
# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers
# for installation instructions
pipe.enable_xformers_memory_efficient_attention()

pipe.enable_model_cpu_offload()

# Display generated images in a 3x4 grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))
image_index = 0

for i, (key, value) in enumerate(depth_images.items()):
    for j, (subkey, pose) in enumerate(value.items()):
        prompt = ", best quality, extremely detailed"
        prompt_list = [t + prompt for t in ["A modern home"]]
        generator = [torch.Generator(device="cuda").manual_seed(10) for _ in range(len(prompt_list))]
        output = pipe(
            prompt_list,
            pose,
            negative_prompt=["monochrome, lowres, bad anatomy, worst quality, low quality"] * len(prompt_list),
            generator=generator,
            num_inference_steps=20,
        )

        row = i
        col = j
        axes[row, col].imshow(output.images[0])
        axes[row, col].axis('off')
        axes[row, col].set_title(f"Generated - Depth - {subkey} {key}")
        image_index += 1

plt.tight_layout()
plt.show()

# HED
link = './'
imgs = [
    load_image(link + 'images_man.png'),
    load_image(link + 'old_man_paint.jpeg'),
    load_image(link + 'old_man_sketch.jpeg'),
]

image_resolutions = create_resolutions(imgs)

fig, axes = plt.subplots(3, 4, figsize=(15, 10))

for i, (key, value) in enumerate(image_resolutions.items()):
    for j, (subkey, subvalue) in enumerate(value.items()):
        row = i
        col = j
        axes[row, col].imshow(subvalue)
        axes[row, col].axis('off')
        axes[row, col].set_title(f"HED - {subkey} {key}")

plt.tight_layout()
plt.show()

from PIL import Image
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
import torch
from controlnet_aux import HEDdetector
from diffusers.utils import load_image

# ... (Your existing code, including the definition of 'images' and 'image_resolutions')

def create_hed_images(image_resolutions):
    """Applies Depth detection to images of different resolutions."""
    hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
    hed_images = {}

    for i, resolutions in image_resolutions.items():
      hed_images[i] = {}
      for resolution_name, image in resolutions.items():
          image = hed(image)
          hed_images[i][resolution_name] = image

    return hed_images

hed_images = create_hed_images(image_resolutions)

# Display Canny images in a grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))

for i, (key, value) in enumerate(hed_images.items()):
    for j, (subkey, subvalue) in enumerate(value.items()):
        row = i
        col = j
        axes[row, col].imshow(subvalue)
        axes[row, col].axis('off')
        axes[row, col].set_title(f"HED - {subkey} {key}")

plt.tight_layout()
plt.show()

controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-hed", torch_dtype=torch.float16
)

pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

# Remove if you do not have xformers installed
# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers
# for installation instructions
pipe.enable_xformers_memory_efficient_attention()

pipe.enable_model_cpu_offload()

# Display generated images in a 3x4 grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))
image_index = 0

for i, (key, value) in enumerate(hed_images.items()):
    for j, (subkey, pose) in enumerate(value.items()):
        prompt = ", best quality, extremely detailed"
        prompt_list = [t + prompt for t in ["oil painting of handsome old man smoking, masterpiece"]]
        generator = [torch.Generator(device="cuda").manual_seed(10) for _ in range(len(prompt_list))]
        output = pipe(
            prompt_list,
            pose,
            negative_prompt=["monochrome, lowres, bad anatomy, worst quality, low quality"] * len(prompt_list),
            generator=generator,
            num_inference_steps=20,
        )

        row = i
        col = j
        axes[row, col].imshow(output.images[0])
        axes[row, col].axis('off')
        axes[row, col].set_title(f"Generated - HED - {subkey} {key}")
        image_index += 1

plt.tight_layout()
plt.show()

# Scribble
link = './'
imgs = [
    load_image(link + 'miumiu.png'),
    load_image(link + 'bag_paint.jpeg'),
    load_image(link + 'bag_sketch.jpeg'),
]

image_resolutions = create_resolutions(imgs)

fig, axes = plt.subplots(3, 4, figsize=(15, 10))

for i, (key, value) in enumerate(image_resolutions.items()):
    for j, (subkey, subvalue) in enumerate(value.items()):
        row = i
        col = j
        axes[row, col].imshow(subvalue)
        axes[row, col].axis('off')
        axes[row, col].set_title(f"HED - {subkey} {key}")

plt.tight_layout()
plt.show()

from PIL import Image
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
import torch
from controlnet_aux import HEDdetector
from diffusers.utils import load_image

# ... (Your existing code, including the definition of 'images' and 'image_resolutions')

def create_scribble_images(image_resolutions):
    """Applies Depth detection to images of different resolutions."""
    hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
    scribble_images = {}

    for i, resolutions in image_resolutions.items():
      scribble_images[i] = {}
      for resolution_name, image in resolutions.items():
          image = hed(image, scribble=True)
          scribble_images[i][resolution_name] = image

    return scribble_images

scribble_images = create_scribble_images(image_resolutions)

# Display Canny images in a grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))

for i, (key, value) in enumerate(scribble_images.items()):
    for j, (subkey, subvalue) in enumerate(value.items()):
        row = i
        col = j
        axes[row, col].imshow(subvalue)
        axes[row, col].axis('off')
        axes[row, col].set_title(f"HED - {subkey} {key}")

plt.tight_layout()
plt.show()

controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-scribble", torch_dtype=torch.float16
)

pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

# Remove if you do not have xformers installed
# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers
# for installation instructions
pipe.enable_xformers_memory_efficient_attention()

pipe.enable_model_cpu_offload()

# Display generated images in a 3x4 grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))
image_index = 0

for i, (key, value) in enumerate(scribble_images.items()):
    for j, (subkey, pose) in enumerate(value.items()):
        prompt = ", best quality, extremely detailed"
        prompt_list = [t + prompt for t in ["A bag with Nike logo"]]
        generator = [torch.Generator(device="cuda").manual_seed(10) for _ in range(len(prompt_list))]
        output = pipe(
            prompt_list,
            pose,
            negative_prompt=["monochrome, lowres, bad anatomy, worst quality, low quality"] * len(prompt_list),
            generator=generator,
            num_inference_steps=20,
        )

        row = i
        col = j
        axes[row, col].imshow(output.images[0])
        axes[row, col].axis('off')
        axes[row, col].set_title(f"Generated - Scrbble - {subkey} {key}")
        image_index += 1

plt.tight_layout()
plt.show()

# LSD
link = './'
imgs = [
    load_image(link + 'apt.png'),
    load_image(link + 'apt_paint.jpeg'),
    load_image(link + 'apt_sketch.jpeg'),
]

image_resolutions = create_resolutions(imgs)

fig, axes = plt.subplots(3, 4, figsize=(15, 10))

for i, (key, value) in enumerate(image_resolutions.items()):
    for j, (subkey, subvalue) in enumerate(value.items()):
        row = i
        col = j
        axes[row, col].imshow(subvalue)
        axes[row, col].axis('off')
        axes[row, col].set_title(f"LSD - {subkey} {key}")

plt.tight_layout()
plt.show()

from PIL import Image
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
import torch
from controlnet_aux import HEDdetector
from diffusers.utils import load_image

# ... (Your existing code, including the definition of 'images' and 'image_resolutions')

def create_LSD_images(image_resolutions):
    """Applies Depth detection to images of different resolutions."""
    mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')
    LSD_images = {}

    for i, resolutions in image_resolutions.items():
      LSD_images[i] = {}
      for resolution_name, image in resolutions.items():
          image = mlsd(image)
          LSD_images[i][resolution_name] = image

    return LSD_images

LSD_images = create_LSD_images(image_resolutions)

# Display Canny images in a grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))

for i, (key, value) in enumerate(LSD_images.items()):
    for j, (subkey, subvalue) in enumerate(value.items()):
        row = i
        col = j
        axes[row, col].imshow(subvalue)
        axes[row, col].axis('off')
        axes[row, col].set_title(f"LSD - {subkey} {key}")

plt.tight_layout()
plt.show()

controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-mlsd", torch_dtype=torch.float16
)

pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

# Remove if you do not have xformers installed
# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers
# for installation instructions
pipe.enable_xformers_memory_efficient_attention()

pipe.enable_model_cpu_offload()

# Display generated images in a 3x4 grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))
image_index = 0

for i, (key, value) in enumerate(LSD_images.items()):
    for j, (subkey, pose) in enumerate(value.items()):
        prompt = ", best quality, extremely detailed"
        prompt_list = [t + prompt for t in ["A rocket-style building"]]
        generator = [torch.Generator(device="cuda").manual_seed(10) for _ in range(len(prompt_list))]
        output = pipe(
            prompt_list,
            pose,
            negative_prompt=["monochrome, lowres, bad anatomy, worst quality, low quality"] * len(prompt_list),
            generator=generator,
            num_inference_steps=20,
        )

        row = i
        col = j
        axes[row, col].imshow(output.images[0])
        axes[row, col].axis('off')
        axes[row, col].set_title(f"Generated - LSD - {subkey} {key}")
        image_index += 1

plt.tight_layout()
plt.show()

# Seg
link = './'
imgs = [
    load_image(link + 'dog_by_couch.png'),
    load_image(link + 'dog_couch_paint.jpeg'),
    load_image(link + 'dog_couch_sketch.jpeg'),
]

image_resolutions = create_resolutions(imgs)

fig, axes = plt.subplots(3, 4, figsize=(15, 10))

for i, (key, value) in enumerate(image_resolutions.items()):
    for j, (subkey, subvalue) in enumerate(value.items()):
        row = i
        col = j
        axes[row, col].imshow(subvalue)
        axes[row, col].axis('off')
        axes[row, col].set_title(f"SEG - {subkey} {key}")

plt.tight_layout()
plt.show()

palette = np.asarray([
    [0, 0, 0],
    [120, 120, 120],
    [180, 120, 120],
    [6, 230, 230],
    [80, 50, 50],
    [4, 200, 3],
    [120, 120, 80],
    [140, 140, 140],
    [204, 5, 255],
    [230, 230, 230],
    [4, 250, 7],
    [224, 5, 255],
    [235, 255, 7],
    [150, 5, 61],
    [120, 120, 70],
    [8, 255, 51],
    [255, 6, 82],
    [143, 255, 140],
    [204, 255, 4],
    [255, 51, 7],
    [204, 70, 3],
    [0, 102, 200],
    [61, 230, 250],
    [255, 6, 51],
    [11, 102, 255],
    [255, 7, 71],
    [255, 9, 224],
    [9, 7, 230],
    [220, 220, 220],
    [255, 9, 92],
    [112, 9, 255],
    [8, 255, 214],
    [7, 255, 224],
    [255, 184, 6],
    [10, 255, 71],
    [255, 41, 10],
    [7, 255, 255],
    [224, 255, 8],
    [102, 8, 255],
    [255, 61, 6],
    [255, 194, 7],
    [255, 122, 8],
    [0, 255, 20],
    [255, 8, 41],
    [255, 5, 153],
    [6, 51, 255],
    [235, 12, 255],
    [160, 150, 20],
    [0, 163, 255],
    [140, 140, 140],
    [250, 10, 15],
    [20, 255, 0],
    [31, 255, 0],
    [255, 31, 0],
    [255, 224, 0],
    [153, 255, 0],
    [0, 0, 255],
    [255, 71, 0],
    [0, 235, 255],
    [0, 173, 255],
    [31, 0, 255],
    [11, 200, 200],
    [255, 82, 0],
    [0, 255, 245],
    [0, 61, 255],
    [0, 255, 112],
    [0, 255, 133],
    [255, 0, 0],
    [255, 163, 0],
    [255, 102, 0],
    [194, 255, 0],
    [0, 143, 255],
    [51, 255, 0],
    [0, 82, 255],
    [0, 255, 41],
    [0, 255, 173],
    [10, 0, 255],
    [173, 255, 0],
    [0, 255, 153],
    [255, 92, 0],
    [255, 0, 255],
    [255, 0, 245],
    [255, 0, 102],
    [255, 173, 0],
    [255, 0, 20],
    [255, 184, 184],
    [0, 31, 255],
    [0, 255, 61],
    [0, 71, 255],
    [255, 0, 204],
    [0, 255, 194],
    [0, 255, 82],
    [0, 10, 255],
    [0, 112, 255],
    [51, 0, 255],
    [0, 194, 255],
    [0, 122, 255],
    [0, 255, 163],
    [255, 153, 0],
    [0, 255, 10],
    [255, 112, 0],
    [143, 255, 0],
    [82, 0, 255],
    [163, 255, 0],
    [255, 235, 0],
    [8, 184, 170],
    [133, 0, 255],
    [0, 255, 92],
    [184, 0, 255],
    [255, 0, 31],
    [0, 184, 255],
    [0, 214, 255],
    [255, 0, 112],
    [92, 255, 0],
    [0, 224, 255],
    [112, 224, 255],
    [70, 184, 160],
    [163, 0, 255],
    [153, 0, 255],
    [71, 255, 0],
    [255, 0, 163],
    [255, 204, 0],
    [255, 0, 143],
    [0, 255, 235],
    [133, 255, 0],
    [255, 0, 235],
    [245, 0, 255],
    [255, 0, 122],
    [255, 245, 0],
    [10, 190, 212],
    [214, 255, 0],
    [0, 204, 255],
    [20, 0, 255],
    [255, 255, 0],
    [0, 153, 255],
    [0, 41, 255],
    [0, 255, 204],
    [41, 0, 255],
    [41, 255, 0],
    [173, 0, 255],
    [0, 245, 255],
    [71, 0, 255],
    [122, 0, 255],
    [0, 255, 184],
    [0, 92, 255],
    [184, 255, 0],
    [0, 133, 255],
    [255, 214, 0],
    [25, 194, 194],
    [102, 255, 0],
    [92, 0, 255],
])

from transformers import AutoImageProcessor, UperNetForSemanticSegmentation
from PIL import Image
import numpy as np
import torch
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from diffusers.utils import load_image

# ... (Your existing code, including the definition of 'images' and 'image_resolutions')

def create_seg_images(image_resolutions):
    """Applies Depth detection to images of different resolutions."""
    image_processor = AutoImageProcessor.from_pretrained("openmmlab/upernet-convnext-small")
    image_segmentor = UperNetForSemanticSegmentation.from_pretrained("openmmlab/upernet-convnext-small")

    seg_images = {}

    for i, resolutions in image_resolutions.items():
      seg_images[i] = {}
      for resolution_name, image in resolutions.items():
          pixel_values = image_processor(image, return_tensors="pt").pixel_values

          with torch.no_grad():
            outputs = image_segmentor(pixel_values)

          seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]

          color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3

          for label, color in enumerate(palette):
              color_seg[seg == label, :] = color

          color_seg = color_seg.astype(np.uint8)

          image = Image.fromarray(color_seg)
          seg_images[i][resolution_name] = image

    return seg_images

seg_images = create_seg_images(image_resolutions)

# Display Canny images in a grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))

for i, (key, value) in enumerate(seg_images.items()):
    for j, (subkey, subvalue) in enumerate(value.items()):
        row = i
        col = j
        axes[row, col].imshow(subvalue)
        axes[row, col].axis('off')
        axes[row, col].set_title(f"SEG - {subkey} {key}")

plt.tight_layout()
plt.show()

controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-seg", torch_dtype=torch.float16
)

pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

# Remove if you do not have xformers installed
# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers
# for installation instructions
pipe.enable_xformers_memory_efficient_attention()

pipe.enable_model_cpu_offload()

!pip install xformers

# Display generated images in a 3x4 grid
fig, axes = plt.subplots(3, 4, figsize=(15, 10))
image_index = 0

for i, (key, value) in enumerate(seg_images.items()):
    for j, (subkey, pose) in enumerate(value.items()):
        prompt = ", best quality, extremely detailed"
        prompt_list = [t + prompt for t in ["A wolf sitting on the couch"]]
        generator = [torch.Generator(device="cuda").manual_seed(10) for _ in range(len(prompt_list))]
        output = pipe(
            prompt_list,
            pose,
            negative_prompt=["monochrome, lowres, bad anatomy, worst quality, low quality"] * len(prompt_list),
            generator=generator,
            num_inference_steps=20,
        )

        row = i
        col = j
        axes[row, col].imshow(output.images[0])
        axes[row, col].axis('off')
        axes[row, col].set_title(f"Generated - SEG - {subkey} {key}")
        image_index += 1

plt.tight_layout()
plt.show()

"""## Conclusion

We have been playing a lot with [`StableDiffusionControlNetPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet), and our experience has been fun so far! We’re excited to see what the community builds on top of this pipeline. If you want to check out other pipelines and techniques supported in Diffusers that allow for controlled generation, check out our [official documentation](https://huggingface.co/docs/diffusers/main/en/using-diffusers/controlling_generation).

If you cannot wait to try out ControlNet directly, we got you covered as well! Simply click on one of the following spaces to play around with ControlNet:
- [![Canny ControlNet Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/diffusers/controlnet-canny)
- [![OpenPose ControlNet Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/diffusers/controlnet-openpose)
"""